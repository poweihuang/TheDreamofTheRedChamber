{
    "collab_server" : "",
    "contents" : "setwd(\"~/Desktop\")\nadzuna <- read.csv(\"Train_rev1.csv\", header=TRUE)\ncomopany <- table(adzuna$SourceName)\nwrite.csv(comopany, file=\"company.csv\")\ncv_library <- subset(adzuna, adzuna$SourceName %in% \"cv-library.co.uk\")\nwrite.csv(cv_library, file=\"cv-library.csv\")\ncategory = table(cv_library$Category)\nlibrary(plyr)\ntrain <- ddply(cv_library, c(\"Id\"), summarise, FullDescription, Category)\ntrain <- data.frame(cv_library)\ntraining <- train[,c(3,9)]\ntraining_des <- train[,c(3)]\nwrite.csv(training, file=\"training\")\n\n#training1 <- do.call(\"rbind\", lapply(training, as.data.frame))\nlibrary(tm)\ndim(training)\n\nmyCorpus <- Corpus(VectorSource(training$FullDescription))\nmyCorpus <- tm_map(myCorpus, content_transformer(tolower), lazy=TRUE)\nremoveURL <- function(x) gsub(\"http[^[:space:]]*\", \"\", x)\nmyCorpus <- tm_map(myCorpus, content_transformer(removeURL))\n# remove punctuation\nmyCorpus <- tm_map(myCorpus, removePunctuation, lazy=TRUE)\n# remove numbers\nmyCorpus <- tm_map(myCorpus, removeNumbers, lazy=TRUE)\n\n#remove anything other than English letters or space\nremoveNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]*\", \"\", x)\nmyCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))\n# remove stopwords\n# keep \"r\" by removing it from stopwords\nmyStopwords <- c(stopwords('english'), \"available\", \"via\", \"will\")\nidx <- which(myStopwords == \"r\")\nmyStopwords <- myStopwords[-idx]\nmyCorpus <- tm_map(myCorpus, removeWords, myStopwords, lazy=TRUE)\n\ndictCorpus <- myCorpus\n# stem words in a text document with the snowball stemmers,\n# which requires packages Snowball, RWeka, rJava, RWekajars\nmyCorpus <- tm_map(myCorpus, stemDocument)\n\n## 75% of the sample size\n#myDtmTFIDF.DT <- data.frame(text=unlist(sapply(myDtmTFIDF, `[`, \"content\")), stringsAsFactors=F)\n#myCorpus.DT <- as.data.frame(inspect(myCorpus))\nsmp_size <- floor((nrow(myDtmTFIDF.DT)/4)*3)\n\n## set the seed to make your partition reproductible\nset.seed(123)\ntrain_ind <- sample(seq_len(nrow(myDtmTFIDF.DT)), size = smp_size)\n\nmyDtmTFIDF_train <- myDtmTFIDF.DT[train_ind, ]\nmyDtmTFIDF_test <- myDtmTFIDF.DT[-train_ind, ]\nmyDtmTFIDF_train <- Corpus(VectorSource(myDtmTFIDF_train))\nmyDtmTFIDF_test <- Corpus(VectorSource(myDtmTFIDF_test))\n# inspect the first three ``documents\"\ninspect(myCorpus[1:3])\n#TF, TF-IDF\nmyDtm <- DocumentTermMatrix(myCorpus, control=list(wordLengths=c(1, Inf),bounds=list(global=c(floor(length(myCorpus)*0.05), Inf))))\n#myDtm_test <- DocumentTermMatrix(myCorpus_test, control=list(wordLengths=c(1,Inf),bounds=list(global=c(floor(length(myCorpus_train)*0.05), Inf))))\nmyDtmTFIDF <- DocumentTermMatrix(myCorpus, control = list(weighting=function(x)weightTfIdf(x, normalize=FALSE),stopwords=TRUE))\n#myDtmTFIDF_train <- removeSparseTerms(myDtmTFIDF_train, 0.7)\n\n#myDtmTFIDF_test <- DocumentTermMatrix(myCorpus_test, control = list(weighting=function(x)weightTfIdf(x, normalize=FALSE),stopwords=TRUE))\n#myDtmTFIDF_test <- removeSparseTerms(myDtmTFIDF_test, 0.7)\nmyDtmTFIDF.DT <- as.data.frame(inspect(myDtmTFIDF))\n\nknn.pred <- summary(knn(myDtmTFIDF_train, myDtmTFIDF_test, category, k=3))\ninspect(myDtmTFIDF_train[266:270,31:40])\n\n\nfindFreqTerms(myDtmTFIDF, lowfreq=10)\nlibrary(wordcloud)\n\n# calculate the frequency of words\nwordcloud(myCorpus, scale=c(4,0.2), max.words=100, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, \"Dark2\"))\n\n\n",
    "created" : 1480741040451.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3914645868",
    "id" : "E62A300C",
    "lastKnownWriteTime" : 1434928842,
    "last_content_update" : 1434928842,
    "path" : "~/R/Adzuna/Adzuna.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}